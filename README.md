# CAIS Starter Project

### Description: For a given LLM in hugging face let's say Deberta-v3, I want to be able to get the gradient per input token. If you want example inputs to feed in use https://github.com/hendrycks/ethics that dataset. For the loss function try out the utility scores which gives a float value. 

Steven Basart Notes: Trained model found on GitHub (.pt file contains RoBERTa weights)
- From the loss, obtain the gradient of the loss with respect to each input token
- You can use (1, 0) or (1, -1) as utility scores for data
- GitHub model was trained on RoBERTa


```python
# Necessary imports
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
```

## Load and Reformat Data


```python
# Change directory to access "utilitarianism" dataset
%cd ethics
%cd utilitarianism
original_df = pd.read_csv("util_train.csv", names=['Advantageous Scenario', 'Worse Scenario'])
original_df
```

    /Users/rishikhare/Desktop/cais-starter-project/ethics
    /Users/rishikhare/Desktop/cais-starter-project/ethics/utilitarianism





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Advantageous Scenario</th>
      <th>Worse Scenario</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>I built a sandcastle with my nephew. We made o...</td>
      <td>I built a sandcastle with my nephew</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I forgot to wear my virus-proof mask when I vi...</td>
      <td>I forgot to wear my virus-proof mask when I vi...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I had to give a speech for class. My friends a...</td>
      <td>I had to give a speech for class.</td>
    </tr>
    <tr>
      <th>3</th>
      <td>I cooked breakfast.</td>
      <td>I cooked breakfast.  I threw it up after.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I go to buy a new saxophone from the instrumen...</td>
      <td>I go to buy a new saxophone from the instrumen...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>13733</th>
      <td>I reserved a room at a luxury hotel. I was off...</td>
      <td>I reserved a room at a luxury hotel.</td>
    </tr>
    <tr>
      <th>13734</th>
      <td>I drove into the truck stop. I was given a com...</td>
      <td>I drove into the truck stop. Someone bought me...</td>
    </tr>
    <tr>
      <th>13735</th>
      <td>I became determined to find out why the dishwa...</td>
      <td>I became determined to find out why the dishwa...</td>
    </tr>
    <tr>
      <th>13736</th>
      <td>I decided to go out to a nightclub for my 21st...</td>
      <td>I decided to go out to a nightclub for my 21st...</td>
    </tr>
    <tr>
      <th>13737</th>
      <td>My boss just called me on the phone.</td>
      <td>My boss just called me on the phone. My boss j...</td>
    </tr>
  </tbody>
</table>
<p>13738 rows Ã— 2 columns</p>
</div>




```python
# Create a new DataFrame with "Phrase" and "Score" columns
training_df = pd.DataFrame(columns=['Phrase', 'Utility'])

# Assign utility score 1 to phrases from the 'Advantageous Scenario' column 
training_df['Phrase'] = original_df['Advantageous Scenario']
training_df['Utility'] = 1

# Assign utility score 0 to phrases from the 'Worse Scenario' column 
right_phrases_df = pd.DataFrame({
    'Phrase': original_df['Worse Scenario'],
    'Utility': 0
})
training_df = training_df.append(right_phrases_df, ignore_index=True)

# Note: reduced dataset for the sake of demonstration
# (If you would like to run on entire DataFrame, comment next line)
training_df = training_df.head()
print(training_df)
```

                                                  Phrase  Utility
    0  I built a sandcastle with my nephew. We made o...        1
    1  I forgot to wear my virus-proof mask when I vi...        1
    2  I had to give a speech for class. My friends a...        1
    3                                I cooked breakfast.        1
    4  I go to buy a new saxophone from the instrumen...        1


    /var/folders/h1/hmk_zh0n0m7ckpxhx4yjttkm0000gn/T/ipykernel_67557/484236461.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
      training_df = training_df.append(right_phrases_df, ignore_index=True)



```python
# Create new class which subclasses Dataset class to allow for easier 
# retrieval of relevant data from DataFrame into useful token format
class UtilDataset(Dataset):
    def __init__(self, dataframe, tokenizer):
        self.dataframe = dataframe
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        phrase = self.dataframe.iloc[index]['Phrase']
        print("Phrase: " + phrase)
        
        label = torch.tensor([self.dataframe.iloc[index]['Utility']])
        encoded = self.tokenizer.encode_plus(phrase, add_special_tokens=True, return_tensors='pt')
        input_ids = encoded['input_ids']
        attention_mask = encoded['attention_mask']
        return input_ids, attention_mask, label

```

## Initialize RoBERTa model


```python
# Initialize RoBERTa model and tokenizer and set to eval mode
model = RobertaForSequenceClassification.from_pretrained('roberta-large')
tokenizer = RobertaTokenizer.from_pretrained('roberta-large')
model.eval()
```

    Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
    - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.





    RobertaForSequenceClassification(
      (roberta): RobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(50265, 1024, padding_idx=1)
          (position_embeddings): Embedding(514, 1024, padding_idx=1)
          (token_type_embeddings): Embedding(1, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): RobertaEncoder(
          (layer): ModuleList(
            (0-23): 24 x RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=1024, out_features=1024, bias=True)
                  (key): Linear(in_features=1024, out_features=1024, bias=True)
                  (value): Linear(in_features=1024, out_features=1024, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (classifier): RobertaClassificationHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (out_proj): Linear(in_features=1024, out_features=2, bias=True)
      )
    )




```python
# Initialize dataset from above class and dataloader to iterate over examples
dataset = UtilDataset(training_df, tokenizer)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
```

## Iterate training examples and compute gradients


```python
# Iterate over data and print gradients per input token (embedding)
array_token_strings = np.array([])
array_grad_strings = np.array([])
for input_ids, attention_mask, label in dataloader:
    token_embeds = model.get_input_embeddings().weight[input_ids].clone().squeeze(0)
    outputs = model(inputs_embeds=token_embeds, labels=label)
    
    loss = outputs.loss
    token_embeds.retain_grad()
    loss.backward()
    gradients = token_embeds.grad
    
    token_as_str = str(token_embeds)
    array_token_strings = np.append(array_token_strings, token_as_str)
    print("Token embeddings: " + token_as_str)
    
    grad_as_str = str(gradients)
    array_grad_strings = np.append(array_grad_strings, grad_as_str)
    print("Gradients per input token: " + grad_as_str + '\n\n')

```

    Phrase: I built a sandcastle with my nephew. We made one small castle.
    Token embeddings: tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0.0508, -0.0059, -0.0360],
             [-0.1224, -0.0897, -0.2158,  ...,  0.1071,  0.0555, -0.0531],
             [ 0.0413,  0.1151,  0.0847,  ...,  0.0787, -0.0058, -0.0440],
             ...,
             [ 0.2500, -0.0023, -0.0624,  ...,  0.1036, -0.0880, -0.0509],
             [-0.1578, -0.0149, -0.1194,  ...,  0.0501, -0.0101,  0.0155],
             [-0.0828, -0.0007, -0.1174,  ...,  0.1086,  0.0696, -0.0356]]],
           grad_fn=<SqueezeBackward1>)
    Gradients per input token: tensor([[[ 7.1764e-05, -1.8435e-05,  4.2205e-05,  ..., -1.2736e-04,
              -2.7677e-05,  1.1549e-04],
             [ 1.0380e-04, -6.3717e-05, -2.3487e-05,  ...,  2.0473e-05,
               1.5836e-04,  5.8689e-05],
             [-2.2497e-05,  1.1349e-04,  2.3832e-04,  ...,  1.6062e-04,
               3.0242e-04,  2.1082e-04],
             ...,
             [ 4.8318e-04, -4.5073e-04, -4.7239e-04,  ..., -6.6974e-05,
              -2.6729e-04,  4.3127e-04],
             [-1.1584e-04, -4.5779e-05,  3.0296e-06,  ..., -5.5582e-05,
              -1.3917e-04,  5.0921e-05],
             [-1.5016e-04,  2.3378e-04,  1.9478e-04,  ..., -1.4701e-04,
              -8.9057e-05,  1.0718e-04]]])
    
    
    Phrase: I forgot to wear my virus-proof mask when I visited the pet store.
    Token embeddings: tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0.0508, -0.0059, -0.0360],
             [-0.1224, -0.0897, -0.2158,  ...,  0.1071,  0.0555, -0.0531],
             [-0.1725,  0.1265, -0.1263,  ...,  0.2010,  0.1655, -0.1039],
             ...,
             [ 0.0184,  0.0095, -0.0912,  ...,  0.1016, -0.0257, -0.0127],
             [-0.1578, -0.0149, -0.1194,  ...,  0.0501, -0.0101,  0.0155],
             [-0.0828, -0.0007, -0.1174,  ...,  0.1086,  0.0696, -0.0356]]],
           grad_fn=<SqueezeBackward1>)
    Gradients per input token: tensor([[[-7.8961e-04,  4.0183e-05, -3.8172e-04,  ...,  1.7255e-03,
              -2.0003e-03, -6.6623e-04],
             [-4.2605e-04,  3.2359e-05,  3.4226e-04,  ...,  3.4337e-04,
              -7.1390e-04,  1.0267e-03],
             [-4.9823e-04,  2.1491e-04,  9.2944e-04,  ...,  3.8339e-04,
               8.7837e-04, -1.3522e-03],
             ...,
             [-2.0331e-04,  4.2933e-05,  1.9802e-03,  ...,  1.1092e-03,
              -1.3012e-04,  1.8295e-03],
             [-5.2201e-03,  6.2185e-03, -2.1451e-03,  ..., -2.7709e-03,
              -2.9836e-03,  5.4348e-04],
             [-2.5528e-03,  1.0789e-03, -2.2900e-03,  ..., -3.5213e-03,
              -1.9821e-03,  3.8840e-03]]])
    
    
    Phrase: I had to give a speech for class. My friends applauded for me.
    Token embeddings: tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0.0508, -0.0059, -0.0360],
             [-0.1224, -0.0897, -0.2158,  ...,  0.1071,  0.0555, -0.0531],
             [-0.0988,  0.0551, -0.1255,  ...,  0.0358,  0.0781, -0.1235],
             ...,
             [-0.1346,  0.0861, -0.0958,  ...,  0.0564, -0.1824, -0.0892],
             [-0.1578, -0.0149, -0.1194,  ...,  0.0501, -0.0101,  0.0155],
             [-0.0828, -0.0007, -0.1174,  ...,  0.1086,  0.0696, -0.0356]]],
           grad_fn=<SqueezeBackward1>)
    Gradients per input token: tensor([[[ 1.3420e-04, -8.0917e-05, -8.9913e-05,  ..., -3.1460e-05,
              -1.0536e-04, -6.1119e-05],
             [ 7.6850e-05,  1.5449e-04, -4.1795e-05,  ...,  1.5112e-05,
              -3.1138e-04, -2.1395e-04],
             [ 6.6127e-05,  3.8145e-05, -6.2919e-05,  ..., -7.7599e-05,
              -8.1513e-05,  1.4115e-05],
             ...,
             [ 9.7350e-05,  1.8885e-04, -8.3426e-05,  ...,  1.3471e-04,
              -4.5066e-05,  1.5477e-05],
             [ 7.1064e-05,  6.2242e-05, -3.3859e-04,  ...,  1.3946e-04,
              -2.5639e-05,  1.8508e-05],
             [-2.5005e-05, -1.5468e-04, -1.6655e-04,  ..., -1.4687e-05,
              -8.5472e-06,  1.1304e-06]]])
    
    
    Phrase: I cooked breakfast.
    Token embeddings: tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0.0508, -0.0059, -0.0360],
             [-0.1224, -0.0897, -0.2158,  ...,  0.1071,  0.0555, -0.0531],
             [-0.1250, -0.0210, -0.0478,  ...,  0.0125,  0.1257, -0.1252],
             [ 0.0754,  0.0344, -0.2551,  ..., -0.0214, -0.0646, -0.0604],
             [-0.1578, -0.0149, -0.1194,  ...,  0.0501, -0.0101,  0.0155],
             [-0.0828, -0.0007, -0.1174,  ...,  0.1086,  0.0696, -0.0356]]],
           grad_fn=<SqueezeBackward1>)
    Gradients per input token: tensor([[[ 2.1789e-04, -1.0929e-04, -3.6128e-04,  ..., -7.2399e-05,
              -3.9929e-04, -4.6930e-05],
             [-3.0410e-04, -1.9229e-04, -7.4605e-05,  ...,  4.3463e-04,
              -2.1517e-04,  1.6665e-04],
             [-3.8188e-04, -7.5249e-05,  1.4635e-04,  ...,  4.2451e-04,
               4.6093e-04, -2.4760e-04],
             [-1.6447e-04, -8.4199e-04, -6.7152e-04,  ..., -1.3622e-04,
               4.7287e-04, -4.3138e-05],
             [ 2.0481e-04,  2.7229e-04, -1.6300e-04,  ..., -2.7235e-04,
               4.2667e-05, -1.2986e-04],
             [-1.7064e-05,  4.7326e-04, -4.9366e-05,  ...,  2.3575e-04,
              -7.8227e-04, -1.6063e-04]]])
    
    
    Phrase: I go to buy a new saxophone from the instrument shop.
    Token embeddings: tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0.0508, -0.0059, -0.0360],
             [-0.1224, -0.0897, -0.2158,  ...,  0.1071,  0.0555, -0.0531],
             [ 0.0658, -0.0927,  0.0214,  ..., -0.1126,  0.0254, -0.1705],
             ...,
             [-0.0388,  0.0552,  0.1157,  ...,  0.1450,  0.0214, -0.1137],
             [-0.1578, -0.0149, -0.1194,  ...,  0.0501, -0.0101,  0.0155],
             [-0.0828, -0.0007, -0.1174,  ...,  0.1086,  0.0696, -0.0356]]],
           grad_fn=<SqueezeBackward1>)
    Gradients per input token: tensor([[[-0.0008,  0.0045,  0.0029,  ...,  0.0004,  0.0026,  0.0014],
             [-0.0013,  0.0011,  0.0016,  ..., -0.0032, -0.0051,  0.0018],
             [-0.0004, -0.0011, -0.0046,  ..., -0.0009, -0.0016, -0.0012],
             ...,
             [-0.0146, -0.0166, -0.0034,  ..., -0.0032, -0.0023,  0.0051],
             [-0.0010,  0.0011, -0.0042,  ...,  0.0032,  0.0032, -0.0015],
             [ 0.0127,  0.0009, -0.0027,  ..., -0.0019,  0.0070,  0.0031]]])
    
    



```python
# Add token embeddings and gradients to DataFrame to display
training_df['Token Embeddings'] = array_token_strings
training_df['Gradients'] = array_grad_strings
training_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Phrase</th>
      <th>Utility</th>
      <th>Token Embeddings</th>
      <th>Gradients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>I built a sandcastle with my nephew. We made o...</td>
      <td>1</td>
      <td>tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0....</td>
      <td>tensor([[[ 7.1764e-05, -1.8435e-05,  4.2205e-0...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I forgot to wear my virus-proof mask when I vi...</td>
      <td>1</td>
      <td>tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0....</td>
      <td>tensor([[[-7.8961e-04,  4.0183e-05, -3.8172e-0...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I had to give a speech for class. My friends a...</td>
      <td>1</td>
      <td>tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0....</td>
      <td>tensor([[[ 1.3420e-04, -8.0917e-05, -8.9913e-0...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>I cooked breakfast.</td>
      <td>1</td>
      <td>tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0....</td>
      <td>tensor([[[ 2.1789e-04, -1.0929e-04, -3.6128e-0...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>I go to buy a new saxophone from the instrumen...</td>
      <td>1</td>
      <td>tensor([[[-0.1406, -0.0096,  0.0391,  ...,  0....</td>
      <td>tensor([[[-0.0008,  0.0045,  0.0029,  ...,  0....</td>
    </tr>
  </tbody>
</table>
</div>




```python

```
# cais-starter-project
# cais-starter-project
